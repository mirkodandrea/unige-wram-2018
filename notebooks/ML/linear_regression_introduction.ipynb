{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML TECHNIQUES FOR REGRESSION\n",
    "\n",
    "_Adapted from_ https://github.com/mubaris/potential-enigma\n",
    "\n",
    "Linear Regression is one of the easiest algorithms in machine learning. In this post we will explore this algorithm and we will implement it using Python from scratch.\n",
    "\n",
    "As the name suggests this algorithm is applicable for Regression problems. Linear Regression is a **Linear Model**. Which means, we will establish a linear relationship between the input variables(**X**) and single output variable(**Y**). When the input(**X**) is a single variable this model is called **Simple Linear Regression** and when there are mutiple input variables(**X**), it is called **Multiple Linear Regression**.\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "We discussed that Linear Regression is a simple model. Simple Linear Regression is the simplest model in machine learning.\n",
    "\n",
    "### Model Representation\n",
    "\n",
    "In this problem we have an input variable - **X** and one output variable - **Y**. And we want to build linear relationship between these variables. Here the input variable is called **Independent Variable** and the output variable is called **Dependent Variable**. We can define this linear relationship as follows:\n",
    "\n",
    "\\\\[Y = \\beta_0 + \\beta_1X\\\\]\n",
    "\n",
    "The \\\\(\\beta_1\\\\) is called a scale factor or **coefficient** and \\\\(\\beta_0\\\\) is called **bias coefficient**. The bias coeffient gives an extra degree of freedom to this model. This equation is similar to the line equation \\\\(y = mx + b\\\\) with \\\\(m = \\beta_1\\\\)(Slope) and \\\\(b = \\beta_0\\\\)(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.\n",
    "\n",
    "But how do we find these coefficients? That's the learning procedure. We can find these using different approaches. One is called **Ordinary Least Square Method** and other one is called **Gradient Descent Approach**. We will use Ordinary Least Square Method in Simple Linear Regression and Gradient Descent Approach in Multiple Linear Regression in post.\n",
    "\n",
    "### Ordinary Least Square Method\n",
    "\n",
    "Earlier in this post we discussed that we are going to approximate the relationship between X and Y to a line. Let's say we have few inputs and outputs. And we plot these scatter points in 2D space, we will get something like the following image.\n",
    "\n",
    "![Linear Regression](https://i.imgur.com/pXEpE6x.png)\n",
    "\n",
    "And you can see a line in the image. That's what we are going to accomplish. And we want to minimize the error of out model. A good model will always have least error. We can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.\n",
    "\n",
    "![Residue](https://i.imgur.com/306wvA1.png)\n",
    "\n",
    "And total error of this model is the sum of all errors of each point. ie.\n",
    "\n",
    "\\\\[D = \\sum_{i=1}^{m} d_i^2\\\\]\n",
    "\n",
    "\\\\(d_i\\\\) - Distance between line and i<sup>th</sup> point.\n",
    "\n",
    "\\\\(m\\\\) - Total number of points\n",
    "\n",
    "You might have noticed that we are squaring each of the distances. This is because, some points will be above the line and some points will be below the line. We can minimize the error in the model by minimizing \\\\(D\\\\). And after the mathematics of minimizing \\\\(D\\\\), we will get;\n",
    "\n",
    "\\\\[\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\\\\]\n",
    "\n",
    "\\\\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\\\\]\n",
    "\n",
    "In these equations \\\\(\\bar{x}\\\\) is the mean value of input variable **X** and \\\\(\\bar{y}\\\\) is the mean value of output variable **Y**.\n",
    "\n",
    "Now we have the model. This method is called [**Ordinary Least Square Method**](https://www.wikiwand.com/en/Ordinary_least_squares). Now we will implement this model in Python.\n",
    "\n",
    "\\\\[Y = \\beta_0 + \\beta_1X\\\\]\n",
    "\n",
    "\\\\[\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\\\\]\n",
    "\n",
    "\\\\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\\\\]\n",
    "\n",
    "### Implementation\n",
    "\n",
    "We are going to use a dataset containing head size and brain weight of different people. This data set has other features. But, we will not use them in this model.. This dataset is available in this [Github Repo](https://github.com/mubaris/potential-enigma). Let's start off by importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age Range</th>\n",
       "      <th>Head Size(cm^3)</th>\n",
       "      <th>Brain Weight(grams)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4512</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3738</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4261</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3777</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4177</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age Range  Head Size(cm^3)  Brain Weight(grams)\n",
       "0       1          1             4512                 1530\n",
       "1       1          1             3738                 1297\n",
       "2       1          1             4261                 1335\n",
       "3       1          1             3777                 1282\n",
       "4       1          1             4177                 1590"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "# Reading Data\n",
    "data = pd.read_csv('./data_ML/headbrain.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 237 values in the training set. We will find a linear relationship between Head Size and Brain Weights. So, now we will get these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting X and Y\n",
    "X = data['Head Size(cm^3)'].values\n",
    "Y = data['Brain Weight(grams)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the values \\\\(\\beta_1\\\\) and \\\\(\\beta_0\\\\), we will need mean of **X** and **Y**. We will find these and the coeffients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26342933948939945 325.57342104944223\n"
     ]
    }
   ],
   "source": [
    "# Mean X and Y\n",
    "mean_x = np.mean(X)\n",
    "mean_y = np.mean(Y)\n",
    "\n",
    "# Total number of values\n",
    "m = len(X)\n",
    "\n",
    "# Using the formula to calculate b1 and b2\n",
    "numer = 0\n",
    "denom = 0\n",
    "for i in range(m):\n",
    "    numer += (X[i] - mean_x) * (Y[i] - mean_y)\n",
    "    denom += (X[i] - mean_x) ** 2\n",
    "b1 = numer / denom\n",
    "b0 = mean_y - (b1 * mean_x)\n",
    "\n",
    "# Print coefficients\n",
    "print(b1, b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have our coefficients.\n",
    "\n",
    "\\\\[Brain Weight = 325.573421049 + 0.263429339489 * Head Size\\\\]\n",
    "\n",
    "That is our linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see this graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Values and Regression Line\n",
    "\n",
    "max_x = np.max(X) + 100\n",
    "min_x = np.min(X) - 100\n",
    "\n",
    "# Calculating line values x and y\n",
    "x = np.linspace(min_x, max_x, 1000)\n",
    "y = b0 + b1 * x\n",
    "\n",
    "# Ploting Line\n",
    "plt.plot(x, y, color='#58b970', label='Regression Line')\n",
    "# Ploting Scatter Points\n",
    "plt.scatter(X, Y, c='#ef5423', label='Scatter Plot')\n",
    "\n",
    "plt.xlabel('Head Size in cm3')\n",
    "plt.ylabel('Brain Weight in grams')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not so bad. But we need to find how good is our model. There are many methods to evaluate models. We will use **Root Mean Squared Error** and **Coefficient of Determination(\\\\(R^2\\\\) Score)**.\n",
    "\n",
    "Root Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,\n",
    "\n",
    "\\\\[RMSE = \\sqrt{\\sum_{i=1}^{m} \\frac{1}{m} (\\hat{y_i} - y_i)^2}\\\\]\n",
    "\n",
    "Here \\\\(\\hat{y_i}\\\\) is the i<sup>th</sup> predicted output values. Now we will find RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.1206213783709\n"
     ]
    }
   ],
   "source": [
    "# Calculating Root Mean Squares Error\n",
    "rmse = 0\n",
    "for i in range(m):\n",
    "    y_pred = b0 + b1 * X[i]\n",
    "    rmse += (Y[i] - y_pred) ** 2\n",
    "rmse = np.sqrt(rmse/m)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find \\\\(R^2\\\\) score. \\\\(R^2\\\\) is defined as follows,\n",
    "\n",
    "\\\\[SS_t = \\sum_{i=1}^{m} (y_i - \\bar{y})^2\\\\]\n",
    "\n",
    "\\\\[SS_r = \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2\\\\]\n",
    "\n",
    "\\\\[R^2 \\equiv 1 - \\frac{SS_r}{SS_t}\\\\]\n",
    "\n",
    "\\\\(SS_t\\\\) is the total sum of squares and \\\\(SS_r\\\\) is the total sum of squares of residuals.\n",
    "\n",
    "\\\\(R^2\\\\) Score usually range from 0 to 1. It will also become negative if the model is completely wrong. Now we will find \\\\(R^2\\\\) Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6393117199570003\n"
     ]
    }
   ],
   "source": [
    "ss_t = 0\n",
    "ss_r = 0\n",
    "for i in range(m):\n",
    "    y_pred = b0 + b1 * X[i]\n",
    "    ss_t += (Y[i] - mean_y) ** 2\n",
    "    ss_r += (Y[i] - y_pred) ** 2\n",
    "r2 = 1 - (ss_r/ss_t)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.63 is not so bad. Now we have implemented Simple Linear Regression Model using Ordinary Least Square Method. Now we will see how to implement the same model using a Machine Learning Library called [scikit-learn](http://scikit-learn.org/)\n",
    "\n",
    "### The scikit-learn approach\n",
    "\n",
    "[scikit-learn](http://scikit-learn.org/) is simple machine learning library in Python. Building Machine Learning models are very easy using scikit-learn. Let's see how we can build this Simple Linear Regression Model using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.1206213783709\n",
      "0.639311719957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Cannot use Rank 1 matrix in scikit learn\n",
    "X = X.reshape((m, 1))\n",
    "# Creating Model\n",
    "reg = LinearRegression()\n",
    "# Fitting training data\n",
    "reg = reg.fit(X, Y)\n",
    "# Y Prediction\n",
    "Y_pred = reg.predict(X)\n",
    "\n",
    "# Calculating RMSE and R2 Score\n",
    "mse = mean_squared_error(Y, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2_score = reg.score(X, Y)\n",
    "\n",
    "print(np.sqrt(mse))\n",
    "print(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this exactly equal to model we built from scratch, but simpler and less code.\n",
    "\n",
    "Now we will move on to Multiple Linear Regression.\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "Multiple Linear Regression is a type of Linear Regression when the input has multiple features(variables).\n",
    "\n",
    "### Model Representation\n",
    "\n",
    "Similar to Simple Linear Regression, we have input variable(**X**) and output variable(**Y**). But the input variable has \\\\(n\\\\) features. Therefore, we can represent this linear model as follows;\n",
    "\n",
    "\\\\[Y = \\beta_0 + \\beta_1x_1 + \\beta_1x_2 + ... + \\beta_nx_n\\\\]\n",
    "\n",
    "\\\\(x_i\\\\) is the i<sup>th</sup> feature in input variable. By introducing \\\\(x_0 = 1\\\\), we can rewrite this equation.\n",
    "\n",
    "\\\\[Y = \\beta_0x_0 + \\beta_1x_1 + \\beta_1x_2 + ... + \\beta_nx_n\\\\]\n",
    "\n",
    "\\\\[x_0 = 1\\\\]\n",
    "\n",
    "Now we can convert this eqaution to matrix form.\n",
    "\n",
    "\\\\[Y = \\beta^TX\\\\]\n",
    "\n",
    "Where,\n",
    "\n",
    "\\\\[\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\.\\\\.\\\\\\beta_n\\end{bmatrix}\\\\]\n",
    "\n",
    "and\n",
    "\n",
    "\\\\[X = \\begin{bmatrix}x_0\\\\x_1\\\\x_2\\\\.\\\\.\\\\x_n\\end{bmatrix}\\\\]\n",
    "\n",
    "We have to define the cost of the model. Cost bascially gives the error in our model. **Y** in above equation is the our hypothesis(approximation). We are going to define it as our hypothesis function.\n",
    "\n",
    "\\\\[h_\\beta(x) = \\beta^Tx\\\\]\n",
    "\n",
    "And the cost is,\n",
    "\n",
    "\\\\[J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\beta(x^{\\textrm{(i)}}) - y^{\\textrm{(i)}})^2\\\\]\n",
    "\n",
    "By minimizing this cost function, we can get find \\\\(\\beta\\\\). We use **Gradient Descent** for this.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.\n",
    "\n",
    "\n",
    "##### Step 1\n",
    "Initialize values \\\\(\\beta_0\\\\), \\\\(\\beta_1\\\\),..., \\\\(\\beta_n\\\\) with some value. In this case we will initialize with 0.\n",
    "\n",
    "#### Step 2\n",
    "\n",
    "Iteratively update,\n",
    "\n",
    "\\\\[\\beta_j := \\beta_j - \\alpha\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\\\\]\n",
    "\n",
    "until it converges.\n",
    "\n",
    "This is the procedure. Here \\\\(\\alpha\\\\) is the learning rate. This operation \\\\(\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\\\\) means we are finding partial derivate of cost with respect to each \\\\(\\beta_j\\\\). This is called Gradient.\n",
    "\n",
    "Read [this](https://math.stackexchange.com/questions/174270/what-exactly-is-the-difference-between-a-derivative-and-a-total-derivative) if you are unfamiliar with partial derivatives.\n",
    "\n",
    "In step 2 we are changing the values of \\\\(\\beta_j\\\\) in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don't want to change values of \\\\(\\beta_j\\\\) drastically, because we might miss the minima. That's why we need learning rate.\n",
    "\n",
    "![Gradient Descent](https://i.imgur.com/xnPvEok.gif)\n",
    "\n",
    "The above animation illustrates the Gradient Descent method.\n",
    "\n",
    "But we still didn't find the value of \\\\(\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\\\\). After we applying the mathematics. The step 2 becomes.\n",
    "\n",
    "\\\\[\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}\\\\]\n",
    "\n",
    "We iteratively change values of \\\\(\\beta_j\\\\) according to above equation. This particular method is called **Batch Gradient Descent**.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's try to implement this in Python. This looks like a long procedure. But the implementation is comparitively easy since we will vectorize all the equations. If you are unfamiliar with vectorization, read this [post](https://www.datascience.com/blog/straightening-loops-how-to-vectorize-data-aggregation-with-pandas-and-numpy/)\n",
    "\n",
    "We will be using a student score dataset. In this particular dataset, we have math, reading and writing exam scores of 1000 students. We will try to find a predict the score of writing exam from math and reading scores. You can get this dataset from this [Github Repo](https://github.com/mubaris/potential-enigma). That's we have 2 features(input variables). Let's start by importing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math  Reading  Writing\n",
       "0    48       68       63\n",
       "1    62       81       72\n",
       "2    79       80       78\n",
       "3    76       83       79\n",
       "4    59       64       62"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "data = pd.read_csv('./data_ML/student.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get scores to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math = data['Math'].values\n",
    "read = data['Reading'].values\n",
    "write = data['Writing'].values\n",
    "\n",
    "# Ploting the scores as scatter plot\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(math, read, write, color='#ef1234')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate our X, Y and \\\\(\\beta\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(math)\n",
    "x0 = np.ones(m)\n",
    "X = np.array([x0, math, read]).T\n",
    "# Initial Coefficients\n",
    "B = np.array([0, 0, 0])\n",
    "Y = np.array(write)\n",
    "alpha = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_cost = cost_function(X, Y, B)\n",
    "print(inital_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our initial cost is huge. Now we'll reduce our cost prediocally using Gradient Descent.\n",
    "\n",
    "**Hypothesis:  \\\\(h_\\beta(x) = \\beta^Tx\\\\)**\n",
    "\n",
    "**Loss: \\\\((h_\\beta(x)-y)\\\\)**\n",
    "\n",
    "**Gradient: \\\\((h_\\beta(x)-y)x_{j}\\\\)**\n",
    "\n",
    "**Gradient Descent Updation: \\\\(\\beta_j := \\beta_j - \\alpha(h_\\beta(x)-y)x_{j})\\\\)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        # Changing Values of B using Gradient\n",
    "        B = B - alpha * gradient\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, B)\n",
    "        cost_history[iteration] = cost\n",
    "        \n",
    "    return B, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute final value of \\\\(\\beta\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100000 Iterations\n",
    "newB, cost_history = gradient_descent(X, Y, B, alpha, 100000)\n",
    "\n",
    "# New Values of B\n",
    "print(newB)\n",
    "\n",
    "# Final Cost of new B\n",
    "print(cost_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that in this model,\n",
    "\n",
    "\\\\[S_{writing} = -0.47889172 + 0.09137252 * S_{math} + 0.90144884 * S_{reading}\\\\]\n",
    "\n",
    "There we have final hypothesis function of our model. Let's calculate **RMSE** and **\\\\(R^2\\\\) Score** of our model to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation - RMSE\n",
    "def rmse(Y, Y_pred):\n",
    "    rmse = np.sqrt(sum((Y - Y_pred) ** 2) / len(Y))\n",
    "    return rmse\n",
    "\n",
    "# Model Evaluation - R2 Score\n",
    "def r2_score(Y, Y_pred):\n",
    "    mean_y = np.mean(Y)\n",
    "    ss_tot = sum((Y - mean_y) ** 2)\n",
    "    ss_res = sum((Y - Y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "Y_pred = X.dot(newB)\n",
    "\n",
    "print(rmse(Y, Y_pred))\n",
    "print(r2_score(Y, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very low value of RMSE score and a good \\\\(R^2\\\\) score. I guess our model was pretty good.\n",
    "\n",
    "Now we will implement this model using scikit-learn.\n",
    "\n",
    "### The scikit-learn Approach\n",
    "\n",
    "scikit-learn approach is very similar to Simple Linear Regression Model and simple too. Let's implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# X and Y\n",
    "X = np.array([math, read]).T\n",
    "Y = np.array(write)\n",
    "\n",
    "# Model Intialization\n",
    "reg = LinearRegression()\n",
    "# Data Fitting\n",
    "reg = reg.fit(X, Y)\n",
    "# Y Prediction\n",
    "Y_pred = reg.predict(X)\n",
    "\n",
    "# Model Evaluation\n",
    "rmse = np.sqrt(mean_squared_error(Y, Y_pred))\n",
    "r2 = reg.score(X, Y)\n",
    "\n",
    "print(rmse)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this model is better than one which we have built from scratch by a small margin.\n",
    "\n",
    "That's it for Linear Regression. I assume, so far you have understood Linear Regression, Ordinary Least Square Method and Gradient Descent.\n",
    "\n",
    "Let me know if you found any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
